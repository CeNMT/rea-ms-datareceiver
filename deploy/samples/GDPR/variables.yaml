# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

###################################################################
########## Variables file for GDPR Aligned Datawarehouse ##########
###################################################################

# DISCLAIMER: Make sure that config.yaml and this file must be in the same folder. DO NOT edit config.yaml

# All the parameter values must be declared against the labels mentioned below after the colons (:)

## To find detailed descriptions for all the parameters for resources used below, visit the following links:
#  https://github.com/GoogleCloudPlatform/healthcare/blob/master/deploy/project_config.yaml.schema
#  https://www.terraform.io/docs/providers/google/

# This is the file that must be run along with the BAZEL build command which is as follows
# bazel run cmd/apply:apply -- \
#   --config_path= ./variables.yaml

## Importing config.yaml to deploy GDPR Aligned Datawarehouse resources using configurations mentioned in this file
# For BigQuery dataset names, use underscore.
# Use region "us-central1" as od healthcare dataset constraints.
imports:
- path: config.yaml

  ## All the parameters that go into the creation of the data warehouse resources are declared in this section
  data:

    ## Overall template parameters [Setting up billing account for the projects in the template]
    BILLING_ACCOUNT:
    DOMAIN:

    # ID of the organization that the projects will be created under
    ORGANIZATION_ID:

    # ID of the organization that the projects will be created under
    # FOLDER_ID:

    # Location used by all the resources to be deployed (Eg. US, EU, etc.)
    LOCATION:

    # Region used by all the resources to be deployed (Eg. us-central1, europe-west4, etc.)
    REGION:

    ## Forseti project parameters [All the parameters required for forseti project]

    # Give a project ID for the forseti project here
    FORSETI_PROJECT_ID:

    # Mention the forseti project's owner and auditor group
    FORSETI_PROJECT_OWNERS_GROUP:
    FORSETI_PROJECT_AUDITORS_GROUP:

    # Mention the ID for the big query dataset that stores forseti project's audit logs
    FORSETI_AUDIT_LOGS_BIGQUERY_DATASET_ID:

    # It's a boolean values (true | false)  to decide if forseti contents get deleted upon destroy
    FORSETI_DELETE_CONTENTS_ON_DESTROY:

    # Describes the rights granted to the user specified by the other member of the access object. Refer to "https://www.terraform.io/docs/providers/google/r/bigquery_dataset.html" for more information.
    FORSETI_AUDIT_LOGS_SPECIAL_GROUP:
    FORSETI_AUDIT_LOGS_SPECIAL_GROUP_ROLE:

    # Name of the private VPC network created for Forseti Resources
    FORSETI_VPC_NETWORK_NAME:

    # Name of the resource that provides private IP Address for Forseti (Not the IP Address itself)
    FORSETI_PRIVATE_IP_NAME:

    # Name for the storage bucket that stores states of resources deployed under forseti (this) project
    FORSETI_STATE_STORAGE_BUCKET:

    # Name of the NAT being used in this project
    FORSETI_ROUTER_NAME:

    # Name of the router being used in this project
    FORSETI_NAT_NAME:

    ## Datawarehouse project parameters [All the parameters required for the project which deployes resources needed in this datawarehouse]
    # Give a project ID for the project deploying the datawarehouse resources
    DATAWAREHOUSE_PROJECT_ID:

    # Mention the datawarehouse project's owner and auditor group
    DATAWAREHOUSE_OWNERS_GROUP:
    DATAWAREHOUSE_AUDITORS_GROUP:

    # Name for the storage bucket that stores states of resources deployed under datawarehouse (this) project
    DATAWAREHOUSE_STATE_STORAGE_BUCKET:

    # Mention the ID for the big query dataset that stores datawarehouse project's audit logs
    DATAWAREHOUSE_AUDIT_LOGS_BIGQUERY_DATASET_ID:

    # Describes the rights granted to the user specified by the other member of the access object. Refer to "https://www.terraform.io/docs/providers/google/r/bigquery_dataset.html" for more information.
    DW_AUDIT_LOGS_SPECIAL_GROUP:
    DW_AUDIT_LOGS_SPECIAL_GROUP_ROLE:

    # Mention the name for the storage nucket that stores datawarehouse project's GCS logs
    DATAWAREHOUSE_GCS_LOGS_STORAGE_BUCKET_NAME:

    # Name of the VPC under which Cloud SQL and it's replica reside
    CLOUD_SQL_VPC_NETWORK_NAME:

    # Name the resource that provides private IP Address for Cloud SQL (Not the IP Address itself)
    CLOUD_SQL_PRIVATE_IP_NAME:

    # Name of the master cloud sql instance
    MASTER_CLOUD_SQL_NAME:

    # Name of the replica of master cloud sql instance
    REPLICA_CLOUD_SQL_NAME:

    # Mention the ID for the big query dataset that stores unprocessed data in the datawarehouse for querying
    RAW_DATA_BIGQUERY_DATASET_ID:

    # Describes the rights granted to the user specified by the other member of the access object. Refer to "https://www.terraform.io/docs/providers/google/r/bigquery_dataset.html" for more information.
    RAW_DATA_BQ_SPECIAL_GROUP:
    RAW_DATA_BQ_SPECIAL_GROUP_ROLE:

    TRANSFORMED_DATA_BQ_SPECIAL_GROUP:
    TRANSFORMED_DATA_BQ_SPECIAL_GROUP_ROLE:

    # Mention the ID for the big query dataset that stores processed data in the datawarehouse for querying
    TRANSFORMED_DATA_BIGQUERY_DATASET_ID:

    # Storage bucket that stores raw data that comes from various healthcare tools used by the organization
    RAW_DATA_STORAGE_BUCKET_NAME:

    # User/group that acts objectcreator for storage bucket that stores raw data that comes from various healthcare tools used by the organization. Prefix the member used eg. user:user@domain if using user.
    RAW_DATA_STORAGE_BUCKET_OBJECTCREATOR:

    # User/group that acts objectviewer for storage bucket that stores raw data that comes from various healthcare tools used by the organization. Prefix the member used eg. user:user@domain if using user.
    RAW_DATA_STORAGE_BUCKET_OBJECTVIEWER:

    # Names for healthcare dataset and it's datastores
    HEALTHCARE_DATASET_NAME:
    HEALTHCARE_DICOM_STORE_NAME:
    HEALTHCARE_FHIR_STORE_NAME:
    HEALTHCARE_HL7V2_STORE_NAME:

    # User/group that acts as viewer for healthcare datastores. Prefix the member used eg. user:user@domain if using user.
    HEALTHCARE_DATASET_VIEWER:

    # Users/groups that act as editor and storage admin for healthcare DICOM datastore respectively. Prefix the member used eg. user:user@domain if using user.
    HEALTHCARE_DICOM_EDITOR:
    HEALTHCARE_DICOM_STOREADMIN:

    # Users/groups that act as reader and editor for healthcare FHIR datastore respectively. Prefix the member used eg. user:user@domain if using user.
    HEALTHCARE_FHIR_STORE_READER:
    HEALTHCARE_FHIR_STORE_EDITOR:

    # Users/groups that act as storage admin, viewer and editor for healthcare HL7V2 datastore respectively. Prefix the member used eg. user:user@domain if using user.
    HEALTHCARE_HL7V2_STOREADMIN:
    HEALTHCARE_HL7V2_INGEST:
    HEALTHCARE_HL7V2_STORE_EDITOR:
