# Copyright 2020 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

######################################################
##### HIPAA AI/ML & Analytics Platform Template ######
######################################################

# This template is parameterized and can't be built directly. Instead, fill in the parametes' values
# in variables.yaml that imports this file

# Make sure that variables.yaml and this file are in the same folder

# To spin up resources for Analytics and AI/ML platform, run the follwing command, provided, variables.yaml is
# filled with resources' configuration values

# bazel run cmd/apply:apply -- \
#   --config_path= ./variables.yaml

# API might need to be enabled during deployment when prompted. Re-apply the command after enabling.
# Try re-running the bazel command if errors are encountered.
# This sections sets up billing account and folder under which the projects must be deployed
overall:
  billing_account: {{.BILLING_ACCOUNT}}
  domain: {{.DOMAIN}}
  organization_id: {{.ORGANIZATION_ID}}
  # folder_id: {{.FOLDER_ID}}

# Path to an empty YAML file in which DPT writes all the generated fields after successful deployment. These fields are used to generate monitoring rules.
generated_fields_path: ./generated_fields.yaml

# Forseti section deploys a forseti image which does security monitoring of GCP resources
forseti:
  project:
    project_id: {{.FORSETI_PROJECT_ID}}
    owners_group: {{.FORSETI_PROJECT_OWNERS_GROUP}}
    auditors_group: {{.FORSETI_PROJECT_AUDITORS_GROUP}}         # Auditors group at project level

    # Bigquery dataset stores audit logs from the forseti project and its resources.
    audit:
      logs_bigquery_dataset:
        dataset_id: {{.FORSETI_AUDIT_LOGS_BIGQUERY_DATASET_ID}}
        delete_contents_on_destroy: {{.FORSETI_DELETE_CONTENTS_ON_DESTROY}}
        access:
        - user_by_email: {{.FORSETIBQ_SERVICE_ACCOUNT_NAME}}@{{.FORSETI_PROJECT_ID}}.iam.gserviceaccount.com
          role: roles/bigquery.dataEditor                # Can provide roles as per requirement.

        location: {{.LOCATION}}
        # default_encryption_configuration:
        #   kms_key_name: (ex.{google_kms_crypto_key.gcs.self_link} or projects/my-project/locations/global/keyRings/my-ring/cryptoKeys/my-key)
        labels:
          data_criticality: medium
          datatype: auditlogs
          project: {{.FORSETI_PROJECT_ID}}

    # Storage bucket stores the terraform states of the resources in the projects.
    devops:
      state_storage_bucket:
        name: {{.FORSETI_STATE_STORAGE_BUCKET}}
        # encryption:
        #   default_kms_key_name: (ex.{google_kms_crypto_key.gcs.self_link} or projects/my-project/locations/global/keyRings/my-ring/cryptoKeys/my-key)
        location: {{.LOCATION}}

    project_services:
    - service: compute.googleapis.com
    - service: servicenetworking.googleapis.com

    # Setup NAT to allow private forseti to access the internet to fetch the Forseti repo while
    # having no external IP.
    # See https://github.com/forseti-security/terraform-google-forseti/issues/234.

    service_accounts:
    - account_id: {{.FORSETIBQ_SERVICE_ACCOUNT_NAME}}

    terraform_deployments:
      resources:
        config:
          resource:
          - google_service_account_iam_member:
              admin-account-iam:
                service_account_id: ${google_service_account.{{.FORSETIBQ_SERVICE_ACCOUNT_NAME}}.name}
                role: roles/iam.serviceAccountUser
                member: group:{{.FORSETI_PROJECT_OWNERS_GROUP}}
          # Setting up VPC network for forseti
          - google_compute_network:
              forseti_private_network:
                name: {{.FORSETI_VPC_NETWORK_NAME}}
                auto_create_subnetworks: false
          - google_compute_subnetwork:
              forseti_subnetwork:
              - name: {{.FORSETI_SUBNETWORK_NAME}}
                network: ${google_compute_network.forseti_private_network.self_link}
                region: {{.REGION}}
                ip_cidr_range: {{.FORSETI_SUBNET_IP_RANGE}} # (ex. 192.168.0.0/20)
          - google_compute_router:
              forseti-router:
                name: {{.FORSETI_ROUTER_NAME}}
                project: {{.FORSETI_PROJECT_ID}}
                network: ${google_compute_network.forseti_private_network.self_link}
                region: {{.REGION}}
          - google_compute_router_nat:
              forseti-nat:
                name: {{.FORSETI_NAT_NAME}}
                project: {{.FORSETI_PROJECT_ID}}
                region: {{.REGION}}
                nat_ip_allocate_option: AUTO_ONLY
                source_subnetwork_ip_ranges_to_nat: ALL_SUBNETWORKS_ALL_IP_RANGES
                router: ${google_compute_router.forseti-router.name}
  properties:
    server_private: true
    client_private: true
    cloudsql_private: true
    network: ${google_compute_network.forseti_private_network.name}
    subnetwork: ${google_compute_subnetwork.forseti_subnetwork.name}

# List of projects deployed for the AI/ML and Analytics platform usecase.
projects:
- project_id: {{.AIML_PROJECT_ID}}
  owners_group: {{.AIML_OWNERS_GROUP}}
  auditors_group: {{.AIML_AUDITORS_GROUP}}

  # Storage bucket stores the terraform states of the resources in the projects.
  devops:
    state_storage_bucket:
      name: {{.AIML_STATE_STORAGE_BUCKET}}
      # encryption:
      #   default_kms_key_name: (ex.{google_kms_crypto_key.gcs.self_link} or projects/my-project/locations/global/keyRings/my-ring/cryptoKeys/my-key)
      location: {{.LOCATION}}
      labels:
        data_criticality: low
        datatype: statefiles
        project: {{.AIML_PROJECT_ID}}

  # Bigquery dataset and data storage bucket to store logs from projects and their resources.
  audit:
    logs_bigquery_dataset:
      dataset_id: {{.AIML_AUDIT_LOGS_BIGQUERY_DATASET_ID}}
      # delete_contents_on_destroy: true
      access:
      - user_by_email: {{.AUDITBQ_SERVICE_ACCOUNT_NAME}}@{{.AIML_PROJECT_ID}}.iam.gserviceaccount.com
        role: roles/bigquery.dataEditor                # Can provide roles as per requirement.
      # default_encryption_configuration:
      #   kms_key_name: (ex.{google_kms_crypto_key.gcs.self_link} or projects/my-project/locations/global/keyRings/my-ring/cryptoKeys/my-key)
      location: {{.LOCATION}}
      labels:
        data_criticality: medium
        datatype: auditlogs
        project: {{.AIML_PROJECT_ID}}
    logs_storage_bucket:
      name: {{.AIML_GCS_LOGS_STORAGE_BUCKET_NAME}}
      location: {{.LOCATION}}
      # encryption:
      #   default_kms_key_name: (ex.{google_kms_crypto_key.gcs.self_link} or projects/my-project/locations/global/keyRings/my-ring/cryptoKeys/my-key)
      # Number of days from the time of creation, after which, storage objects from GCS logs bucket are moved to secondary storage class
      lifecycle_rule:
      - condition:
          age: {{.AIML_GCS_LOGS_AGE_FOR_SECONDARY_STORAGE_CLASS}}
        action:
          type: SetStorageClass
          # The storage class to which, objects from GCS logs bucket will be pushed to, after the specified number of days mentioned above (e.g. STANDARD/REGIONAL/MULTI_REGIONAL/COLDLINE/NEARLINE)
          storage_class: {{.AIML_GCS_LOGS_SECONDARY_STORAGE_CLASS}}
      labels:
        data_criticality: medium
        datatype: gcslogs
        project: {{.AIML_PROJECT_ID}}

  # APIs required by AI/ML tasks and also other necessary APIs are enabled here
  project_services:
  - service: compute.googleapis.com
  - service: servicenetworking.googleapis.com
  - service: dataproc.googleapis.com
  - service: dlp.googleapis.com
  - service: speech.googleapis.com
  - service: lifesciences.googleapis.com
  - service: translate.googleapis.com
  - service: videointelligence.googleapis.com
  - service: vision.googleapis.com
  - service: language.googleapis.com

  # Custom created servive account
  service_accounts:
  - account_id: {{.DATAPROC_SERVICE_ACCOUNT_NAME}}
  - account_id: {{.DATALAB_SERVICE_ACCOUNT_NAME}}
  - account_id: {{.DEEPVM_SERVICE_ACCOUNT_NAME}}
  - account_id: {{.AUDITBQ_SERVICE_ACCOUNT_NAME}}
  - account_id: {{.RAWDATABQ_SERVICE_ACCOUNT_NAME}}
  - account_id: {{.TRANSDATABQ_SERVICE_ACCOUNT_NAME}}

  terraform_deployments:
    resources:
      config:
        module:
        # Refer "https://github.com/terraform-google-modules/terraform-google-datalab" for more information.
        - datalab:
          - datalab_user_email: {{.DATALAB_EMAIL}}
            # adding gpu requires a quota assigned. Refer to link "https://github.com/terraform-google-modules/terraform-google-datalab/tree/master/examples/basic" for furthur information.
            network_name: ${google_compute_network.private_network.name}
            project_id: {{.AIML_PROJECT_ID}}
            source: terraform-google-modules/datalab/google//modules/instance
            subnet_name: ${google_compute_subnetwork.datalab-subnetwork.name}
            version: "~> 1.0"
            zone: {{.ZONE}}
            # custom service account with the deployer given serviceAccountUser role
            service_account: ${google_service_account.{{.DATALAB_SERVICE_ACCOUNT_NAME}}.email}
            datalab_enable_swap: true
            datalab_enable_backup: true
            datalab_console_log_level: "warn"
            datalab_idle_timeout: "60m"
            create_disk: true

        resource:
        - google_compute_snapshot:
            deep-learning-vm-snapshot:
              name: deep-learning-vm-snapshot
              source_disk: ${google_compute_disk.deep-learning-vm-disk.name}
              zone: {{.ZONE}}
              labels:
                project: {{.AIML_PROJECT_ID}}
                connected_disk: ${google_compute_disk.deep-learning-vm-disk.name}
              # Customer Managed Keys must be configured here
              # snapshot_encryption_key:
              #   raw_key:
              #   sha256:
        - google_compute_disk:
            deep-learning-vm-disk:
              name: deep-learning-vm-disk
              type: pd-ssd
              zone: {{.ZONE}}
              labels:
                project: {{.AIML_PROJECT_ID}}
                connected_instance: {{.DEEP_LEARNING_VM_NAME}}
              physical_block_size_bytes: 4096
              # Customer Managed Keys must be configured here
              # disk_encryption_key:
              #   raw_key:
              #   sha256:
              #   kms_key_self_link: (ex.{google_kms_crypto_key.gcs.self_link} or projects/my-project/locations/global/keyRings/my-ring/cryptoKeys/my-key)
        - google_compute_subnetwork:
          - datalab-subnetwork:
            - name: datalab-subnet
              network: ${google_compute_network.private_network.self_link}
              region: {{.REGION}}
              ip_cidr_range: 10.2.0.0/16
          - deep-learning-vm-subnetwork:
            - name: deep-learning-vm-subnet
              network: ${google_compute_network.private_network.self_link}
              region: {{.REGION}}
              ip_cidr_range: 10.3.0.0/16
        # User provided with serviceAccountUser role on service account
        - google_service_account_iam_member:
            dataproc:
              service_account_id: ${google_service_account.{{.DATAPROC_SERVICE_ACCOUNT_NAME}}.name}
              role: roles/iam.serviceAccountUser
              member: group:{{.AIML_OWNERS_GROUP}}
            datalab:
              service_account_id: ${google_service_account.{{.DATALAB_SERVICE_ACCOUNT_NAME}}.name}
              role: roles/iam.serviceAccountUser
              member: group:{{.AIML_OWNERS_GROUP}}
            deep-learning-vm:
              service_account_id: ${google_service_account.{{.DEEPVM_SERVICE_ACCOUNT_NAME}}.name}
              role: roles/iam.serviceAccountUser
              member: group:{{.AIML_OWNERS_GROUP}}
            auditbq:
              service_account_id: ${google_service_account.{{.AUDITBQ_SERVICE_ACCOUNT_NAME}}.name}
              role: roles/iam.serviceAccountUser
              member: group:{{.AIML_OWNERS_GROUP}}
            rawdatabq:
              service_account_id: ${google_service_account.{{.RAWDATABQ_SERVICE_ACCOUNT_NAME}}.name}
              role: roles/iam.serviceAccountUser
              member: group:{{.AIML_OWNERS_GROUP}}
            transdatabq:
              service_account_id: ${google_service_account.{{.TRANSDATABQ_SERVICE_ACCOUNT_NAME}}.name}
              role: roles/iam.serviceAccountUser
              member: group:{{.AIML_OWNERS_GROUP}}
        # Service account provided with dataproc worker role on project
        - google_project_iam_member:
            projects-dataproc:
              project: {{.AIML_PROJECT_ID}}
              role: roles/dataproc.worker
              member: serviceAccount:{{.DATAPROC_SERVICE_ACCOUNT_NAME}}@{{.AIML_PROJECT_ID}}.iam.gserviceaccount.com
        # Dataproc cluster
        - google_dataproc_cluster:
            mycluster:
              name: {{.DATAPROC_CLUSTER_NAME}}
              region: {{.REGION}}
              labels:
                project: {{.AIML_PROJECT_ID}}
              cluster_config:
                master_config:
                  num_instances: 1
                  machine_type: n1-standard-1
                  disk_config:
                    boot_disk_type: pd-ssd
                    boot_disk_size_gb: 15
                worker_config:
                  num_instances: 2
                  machine_type: n1-standard-1
                  min_cpu_platform: Intel Skylake
                  disk_config:
                    boot_disk_size_gb: 15
                    num_local_ssds: 1
                preemptible_worker_config:
                  num_instances: 0
                software_config:
                  image_version: 1.3.7-deb9
                  override_properties:
                    dataproc:dataproc.allow.zero.workers: 'true'
                # KMS keys configuration can be enabled. Refer "https://www.terraform.io/docs/providers/google/r/dataproc_cluster.html"
                # security_config:
                #   kerberos_config:
                #     kms_key_uri: (ex.{google_kms_crypto_key.gcs.self_link} or projects/my-project/locations/global/keyRings/my-ring/cryptoKeys/my-key)
                #     root_principal_password_uri: bucketId/o/objectId
                autoscaling_config:
                  policy_uri: ${google_dataproc_autoscaling_policy.asp.name}
                gce_cluster_config:
                  service_account: {{.DATAPROC_SERVICE_ACCOUNT_NAME}}@{{.AIML_PROJECT_ID}}.iam.gserviceaccount.com
        - google_dataproc_autoscaling_policy:
            asp:
              policy_id: dataprocs-policys
              location: {{.REGION}}
              worker_config:
                min_instances: 2
                max_instances: 5
              basic_algorithm:
                yarn_config:
                  graceful_decommission_timeout: 30s
                  scale_up_factor: 0.5
                  scale_down_factor: 0.5
        # IAM for dataprocs
        - google_dataproc_cluster_iam_member:
          - editor:
            - cluster: ${google_dataproc_cluster.mycluster.name}
              member: {{.DATAPROC_CLUSTER_EDITOR_ROLE_MEMBER}}
              role: roles/editor
              region: {{.REGION}}
          - viewer:
            - cluster: ${google_dataproc_cluster.mycluster.name}
              member: {{.DATAPROC_CLUSTER_VIEWER_ROLE_MEMBER}}
              role: roles/viewer
              region: {{.REGION}}
        - google_dataproc_job_iam_member:
          - editor:
            - job_id: ${google_dataproc_job.pyspark.reference[0].job_id}
              member: {{.DATAPROC_JOB_EDITOR_ROLE_MEMBER}}
              role: roles/editor
              region: {{.REGION}}
          - viewer:
            - job_id: ${google_dataproc_job.pyspark.reference[0].job_id}
              member: {{.DATAPROC_JOB_VIEWER_ROLE_MEMBER}}
              role: roles/viewer
              region: {{.REGION}}
        # Example pyspark dataproc job. Refer "https://www.terraform.io/docs/providers/google/r/dataproc_job.html" for more examples.
        - google_dataproc_job:
            pyspark:
              region: ${google_dataproc_cluster.mycluster.region}
              force_delete: true
              placement:
                cluster_name: ${google_dataproc_cluster.mycluster.name}
              pyspark_config:
                main_python_file_uri: gs://dataproc-examples-2f10d78d114f6aaec76462e3c310f31f/src/pyspark/hello-world/hello-world.py
                properties:
                  spark.logConf: 'true'
        # Setting up VPC network for Datalab, Cloud SQL and Deep Learning VM
        - google_compute_network:
            private_network:
              name: {{.PRIVATE_VPC_NETWORK_NAME}}
              auto_create_subnetworks: false
        - google_compute_global_address:
            private_ip_addresses:
              name: {{.CLOUD_SQL_PRIVATE_IP_NAME}}
              purpose: VPC_PEERING
              address_type: INTERNAL
              prefix_length: 16
              network: ${google_compute_network.private_network.self_link}
        - google_service_networking_connection:
            private_vpc_connection:
              network: ${google_compute_network.private_network.self_link}
              service: servicenetworking.googleapis.com
              reserved_peering_ranges:
              - ${google_compute_global_address.private_ip_addresses.name}
        # Setting up master and replica SQL instances
        - google_sql_database_instance:
            instance:
              name: {{.MASTER_CLOUD_SQL_NAME}}
              region: {{.REGION}}
              depends_on:
              - google_service_networking_connection.private_vpc_connection
              settings:
                availability_type: ZONAL
                tier: db-f1-micro
                ip_configuration:
                  ipv4_enabled: false
                  private_network: ${google_compute_network.private_network.self_link}
                backup_configuration:
                  binary_log_enabled: true
                  enabled: true
            replica-instance:
              name: {{.REPLICA_CLOUD_SQL_NAME}}
              region: {{.REGION}}
              master_instance_name: ${google_sql_database_instance.instance.name}
              depends_on:
              - google_service_networking_connection.private_vpc_connection
              settings:
                availability_type: ZONAL
                tier: db-f1-micro
                ip_configuration:
                  ipv4_enabled: false
                  private_network: ${google_compute_network.private_network.self_link}
              replica_configuration:
                failover_target: true
                master_heartbeat_period: 60000
  # Deploying 2 BigQuery datasets for raw and transformed data
  bigquery_datasets:
  - dataset_id: {{.RAW_DATA_BIGQUERY_DATASET_ID}}
    # delete_contents_on_destroy: true
    depends_on:
    - google_service_account.{{.RAWDATABQ_SERVICE_ACCOUNT_NAME}}
    access:
    - user_by_email: ${google_service_account.{{.RAWDATABQ_SERVICE_ACCOUNT_NAME}}.email}
      role: roles/bigquery.dataEditor                # Can provide roles as per requirement.
    - special_group: {{.RAW_DATA_BQ_SPECIAL_GROUP}}
      role: {{.RAW_DATA_BQ_SPECIAL_GROUP_ROLE}}
    # default_encryption_configuration:
    #   kms_key_name: (ex.{google_kms_crypto_key.gcs.self_link} or projects/my-project/locations/global/keyRings/my-ring/cryptoKeys/my-key)
    location: {{.LOCATION}}
    labels:
      data_criticality: {{.RAW_DATA_BIGQUERY_DATASET_DATA_CRITICALITY_LABEL}}
      datatype: {{.RAW_DATA_BIGQUERY_DATASET_DATA_TYPE_LABEL}}
      project: {{.AIML_PROJECT_ID}}

  - dataset_id: {{.TRANSFORMED_DATA_BIGQUERY_DATASET_ID}}
    delete_contents_on_destroy: true
    depends_on:
    -  google_service_account.{{.TRANSDATABQ_SERVICE_ACCOUNT_NAME}}
    access:
    - user_by_email: ${google_service_account.{{.TRANSDATABQ_SERVICE_ACCOUNT_NAME}}.email}
      role: roles/bigquery.dataEditor                # Can provide roles as per requirement.
    - special_group: {{.TRANSFORMED_DATA_BQ_SPECIAL_GROUP}}
      role: {{.TRANSFORMED_DATA_BQ_SPECIAL_GROUP_ROLE}}
    # default_encryption_configuration:
    #   kms_key_name: (ex.{google_kms_crypto_key.gcs.self_link} or projects/my-project/locations/global/keyRings/my-ring/cryptoKeys/my-key)
    location: {{.LOCATION}}
    labels:
      data_criticality: {{.TRANSFORMED_DATA_BIGQUERY_DATASET_DATA_CRITICALITY_LABEL}}
      datatype: {{.TRANSFORMED_DATA_BIGQUERY_DATASET_DATA_TYPE_LABEL}}
      project: {{.AIML_PROJECT_ID}}
  # Deploying storage bucket as a data source
  storage_buckets:
  - name: {{.RAW_DATA_STORAGE_BUCKET_NAME}}
    # IAM Role
    _iam_members:
    - role: roles/storage.objectCreator
      member: {{.RAW_DATA_STORAGE_BUCKET_OBJECTCREATOR}}
    - role: roles/storage.objectViewer
      member: {{.RAW_DATA_STORAGE_BUCKET_OBJECTVIEWER}}
    # encryption:
    #   default_kms_key_name: (ex.{google_kms_crypto_key.gcs.self_link} or projects/my-project/locations/global/keyRings/my-ring/cryptoKeys/my-key)
    location: {{.LOCATION}}
    labels:
      data_criticality: {{.RAW_DATA_STORAGE_BUCKET_DATA_CRITICALITY_LABEL}}
      datatype: {{.RAW_DATA_STORAGE_BUCKET_DATA_TYPE_LABEL}}
      project: {{.AIML_PROJECT_ID}}
  # Datastores for personal data which aid in features of Healthcare API
  healthcare_datasets:
  - name: {{.HEALTHCARE_DATASET_NAME}}
    location: {{.REGION}}
    # IAM Role
    _iam_members:
    - role: roles/healthcare.datasetViewer
      member: {{.HEALTHCARE_DATASET_VIEWER}}
    _dicom_stores:
    - name: {{.HEALTHCARE_DICOM_STORE_NAME}}
      _iam_members:
      - role: roles/healthcare.dicomEditor
        member: {{.HEALTHCARE_DICOM_EDITOR}}
      - role: roles/healthcare.dicomStoreAdmin
        member: {{.HEALTHCARE_DICOM_STOREADMIN}}
    _fhir_stores:
    - name: {{.HEALTHCARE_FHIR_STORE_NAME}}
      _iam_members:
      - role: roles/healthcare.fhirResourceReader
        member: {{.HEALTHCARE_FHIR_STORE_READER}}
      - role: roles/healthcare.fhirResourceEditor
        member: {{.HEALTHCARE_FHIR_STORE_EDITOR}}
    _hl7_v2_stores:
    - name: {{.HEALTHCARE_HL7V2_STORE_NAME}}
      _iam_members:
      - role: roles/healthcare.hl7V2StoreAdmin
        member: {{.HEALTHCARE_HL7V2_STOREADMIN}}
      - role: roles/healthcare.hl7V2Ingest
        member: {{.HEALTHCARE_HL7V2_INGEST}}
      - role: roles/healthcare.hl7V2Editor
        member: {{.HEALTHCARE_HL7V2_STORE_EDITOR}}
  # Deep Learning VMs
  compute_instances:
  - name: {{.DEEP_LEARNING_VM_NAME}}
    labels:
      data_criticality: {{.DEEP_LEARNING_VM_DATA_CRITICALITY_LABEL}}
      datatype: {{.DEEP_LEARNING_VM_DATA_TYPE_LABEL}}
      project: {{.AIML_PROJECT_ID}}
    zone: {{.ZONE}}
    machine_type: n1-standard-1
    attached_disk:
    - source: ${google_compute_disk.deep-learning-vm-disk.self_link}
      mode: READ_ONLY
      # A 256-bit customer supported key stored with them.
      # disk_encryption_key_raw:
      # A key stored on Google Cloud KMS.
      #   kms_key_self_link: (ex.{google_kms_crypto_key.gcs.self_link} or projects/my-project/locations/global/keyRings/my-ring/cryptoKeys/my-key)
    boot_disk:
      auto_delete: false
      mode: READ_WRITE    # can be changed to READ_WRITE or READ_ONLY
      initialize_params:
        # deep learning vm containing tensorflow pre-installed as example. Use "gcloud compute images list --project deeplearning-platform-release --no-standard-images" to find list of more VMs users can use.
        # using VMs with GPU requires specific congigurations. Check the deep learning VM Google documentation for the same.
        image: https://www.googleapis.com/compute/v1/projects/deeplearning-platform-release/global/images/tf2-latest-cpu-20200227
      # A 256-bit customer supported key stored with them.
      # disk_encryption_key_raw:
      # A key stored on Google Cloud KMS.
      # kms_key_self_link: (ex.{google_kms_crypto_key.gcs.self_link} or projects/my-project/locations/global/keyRings/my-ring/cryptoKeys/my-key)
    network_interface:
      subnetwork: ${google_compute_subnetwork.deep-learning-vm-subnetwork.self_link}
    service_account:
      email: ${google_service_account.{{.DEEPVM_SERVICE_ACCOUNT_NAME}}.email}
      scopes:
      - bigquery
      - sql-admin
      - userinfo-email
      - compute-ro
      - storage-ro
    _iam_members:
    - role: roles/editor
      member: {{.DEEP_LEARNING_VM_EDITOR_ROLE_MEMBER}}
    - role: roles/viewer
      member: {{.DEEP_LEARNING_VM_VIEWER_ROLE_MEMBER}}
    # allow_stopping_for_update: true / false          #  Enables an instance to be stopped for an updation purpose)
    deletion_protection: false
    # shielded_instance_config:                        #  Shielded VM Config can only be set when using a UEFI-compatible disk
    # enable_secure_boot: true
